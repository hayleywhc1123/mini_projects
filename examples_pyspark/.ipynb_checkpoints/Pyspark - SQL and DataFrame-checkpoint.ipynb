{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL and DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ***SparkSession*** can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files. To create a SparkSession, use the following builder pattern:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***getOrCreate***: Gets an existing SparkSession or, if there is no existing one, creates a new one based on the options set in this builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf=SparkConf().set(\"spark.python.profile\", \"true\")\n",
    "spark=SparkSession.builder.master(\"local\").appName(\"wordcount\").config(conf=SparkConf()).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***createDataFrame***(data, schema=None, samplingRatio=None, verifySchema=True)\n",
    "\n",
    "Creates a DataFrame from an RDD, a list or a pandas.DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creates a DataFrame from a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='alice', age=1), Row(name='bob', age=2)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=[(\"alice\",1),(\"bob\",2)]\n",
    "df=spark.createDataFrame(l,['name','age'])\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alice</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bob</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name  age\n",
       "0  alice    1\n",
       "1    bob    2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creates a DataFrame from pandas DataFrame, and use sql query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='alice', age=1), Row(name='bob', age=2)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df2=spark.createDataFrame(pd.DataFrame(l,columns=['name','age']))\n",
    "df2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='alice'), Row(name='bob')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.select(\"name\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='alice'), Row(name='bob')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.createOrReplaceTempView('table1')\n",
    "spark.sql(\"select name from table1\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='alice', age=1), Row(name='bob', age=2)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.table(\"table1\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQLContext\n",
    "\n",
    "The entry point for working with structured data (rows and columns) in Spark, in Spark 1.x.\n",
    "\n",
    "As of Spark 2.0, this is replaced by SparkSession. However, we are keeping the class here for backward compatibility.\n",
    "\n",
    "A SQLContext can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='alice', age=1), Row(name='bob', age=2)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row,SQLContext\n",
    "sqlContext=SQLContext(sc)\n",
    "rdd=sc.parallelize(l)\n",
    "Person=Row(\"name\",\"age\")\n",
    "person=rdd.map(lambda x: Person(*x))\n",
    "sqlContext.createDataFrame(person).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkETL \n",
    "\n",
    "ETL is a type of data integration process referring to three distinct but interrelated steps (Extract, Transform and Load) and is used to synthesize data from multiple sources many times to build a Data Warehouse, Data Hub, or Data Lake.\n",
    "\n",
    "Let's write an ETL job on pyspark!\n",
    "\n",
    "Reference: https://github.com/AlexIoannides/pyspark-example-project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building ETL process, we write a function *start_spark* to start our sparkSession, update and get our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import __main__\n",
    "\n",
    "from os import environ,listdir,path\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "import json\n",
    "\n",
    "\n",
    "def start_spark(app_name=\"my_spark_app\",master=\"local[*]\",files=['etl_conf.json']):\n",
    "\n",
    "\n",
    "    flag_repl=not(hasattr(__main__,'__file__'))\n",
    "    flag_debug='DEBUG' in environ.keys()\n",
    "        \n",
    "        \n",
    "    if not(flag_repl or flag_debug):    \n",
    "        spark_builder=(SparkSession.builder.appName(app_name))\n",
    "    else:\n",
    "        spark_builder=SparkSession.builder.appName(app_name).master(master)\n",
    "    \n",
    "    spark_files='.'.join(list(files))\n",
    "    spark_builder.config('spark.files',spark_files)\n",
    "    spark_builder.config(conf=SparkConf())    \n",
    "    spark_sess=spark_builder.getOrCreate()\n",
    "    \n",
    "    \n",
    "    #spark_logger=logger.Log4j(spark_sess)\n",
    "    spark_files_dir=SparkFiles.getRootDirectory()\n",
    "    \n",
    "    config_files=[x for x in listdir(spark_files_dir) if x.endswith('conf.json')]\n",
    "    \n",
    "    if config_files:\n",
    "        path_to_config_file=path.join(spark_files_dir,config_files[0])\n",
    "        with open(path_to_config_file,'r') as f:\n",
    "            config_dict=json.load(f)\n",
    "    else:\n",
    "        \n",
    "        config_dict=None\n",
    "    \n",
    "    return spark_sess,config_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETL Process contains 3 stages: Extract, Transform, Load. In Spark,\n",
    "- **Extract**: read Parquet format data in local machine\n",
    "- **Transform**: use sparkSQL to manipulate data\n",
    "- **Load**: write to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark import start_spark\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "def main():\n",
    "    spark,conf=start_spark()\n",
    "    steps_per_floor_=conf['steps_per_floor']\n",
    "    df=extract(spark)\n",
    "    df_tf=transform(df,steps_per_floor_,spark)\n",
    "    load(df_tf)\n",
    "\n",
    "def extract(spark):\n",
    "    df=spark.read.parquet('tests/test_data/employees')\n",
    "    return df\n",
    "\n",
    "def transform(df,steps_per_floor_,spark):\n",
    "    df.createOrReplaceTempView(\"table1\")\n",
    "    df_transformed=spark.sql(\"select id, concat(first_name,' ' , second_name) as name, floor* %s as steps_to_desk from table1\"%steps_per_floor_)\n",
    "    return df_transformed\n",
    "\n",
    "def load(df):\n",
    "    df.coalesce(1).write.csv('loaded_data', mode='overwrite', header=True)\n",
    "\n",
    "def create_test_data(spark,conf):   \n",
    "    local_records=[\n",
    "            Row(id=1, first_name='nancy', second_name=\"yan\", floor=1),\n",
    "            Row(id=2, first_name='Dan', second_name='Sommerville', floor=1),\n",
    "            Row(id=3, first_name='Alex', second_name='Ioannides', floor=2),\n",
    "            Row(id=4, first_name='Ken', second_name='Lai', floor=2),\n",
    "            Row(id=5, first_name='Stu', second_name='White', floor=3),\n",
    "            Row(id=6, first_name='Mark', second_name='Sweeting', floor=3),\n",
    "            Row(id=7, first_name='Phil', second_name='Bird', floor=4),\n",
    "            Row(id=8, first_name='Kim', second_name='Suter', floor=4)\n",
    "        ]\n",
    "    \n",
    "    df=spark.createDataFrame(local_records)\n",
    "    df_tf=transform(df,conf['steps_per_floor'],spark)\n",
    "    df_tf.coalesce(1).write.parquet('tests/test_data/employees_report',mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
